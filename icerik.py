# icerik.py (uyumlu + geliÅŸmiÅŸ)
import csv
import requests
from bs4 import BeautifulSoup
from time import sleep
from urllib.parse import urljoin, urlparse
import re

HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

KONULAR_CSV = "konular.csv"
OUT_CSV = "cevaplar.csv"


# ---------------------------------------------------
#   GELÄ°ÅMÄ°Å TEMÄ°ZLEME FONKSÄ°YONU
# ---------------------------------------------------
def clean_text(s):
    """MesajlarÄ± temizle, quote kalÄ±ntÄ±larÄ±nÄ± sil, gereksiz meta/ÅŸablonlarÄ± Ã§Ä±kar."""
    if not s:
        return ""

    # Ã§oklu boÅŸluklarÄ± dÃ¼zelt
    s = re.sub(r'\s+', ' ', s).strip()

    # mesaj linki â€“ ÅŸikayet â€“ meta
    s = re.sub(r'Mesaj Linkini Kopyala.*?', '', s, flags=re.I)
    s = re.sub(r'Åikayet.*?', '', s, flags=re.I)

    # gizli DH meta mesajlarÄ±
    s = re.sub(r'< Bu mesaj .*?>', '', s)
    s = re.sub(r'Bu mesaj bir yÃ¶netici tarafÄ±ndan.*', '', s)

    # quote kalÄ±ntÄ±larÄ±
    s = re.sub(r'.*AlÄ±ntÄ±.*', '', s)

    # link temizliÄŸi
    s = re.sub(r'http\S+', '', s)

    # Ã§ok uzun aÅŸÄ±rÄ± hikayeleri filtrele (spam Ã¶nleme)
    if len(s) > 1200:
        return ""

    return s.strip()


# ---------------------------------------------------
#   SAYFALANDIRMA TESPÄ°TÄ° (ESKÄ°YLE UYUMLU)
# ---------------------------------------------------
def get_thread_pages(soup, base_url):
    pages = set([base_url])

    selectors = [
        "div.topic-pages a",
        "div.paging a",
        "div.paginator a",
        "ul.pagination a",
        "nav.pagination a",
        "div.sayfalar a",
    ]

    for sel in selectors:
        for a in soup.select(sel):
            href = a.get("href")
            if not href:
                continue
            full = urljoin(base_url, href)
            if urlparse(full).path.startswith(urlparse(base_url).path):
                pages.add(full)

    pages = list(pages)
    pages.sort()
    return pages


# ---------------------------------------------------
#   QUOTE (ALINTI) TEMÄ°ZLEYÄ°CÄ° â€“ YENÄ° EKLENDÄ°
# ---------------------------------------------------
def remove_quotes(block):
    """DH forum quote bÃ¶lÃ¼mlerini tamamen DOM'dan sÃ¶ker."""
    for q in block.select(".quote"):
        q.decompose()
    for q in block.select(".msg-quote"):
        q.decompose()
    return block


# ---------------------------------------------------
#   MESAJ AYIKLAMA (ESKÄ°YLE UYUMLU + GELÄ°ÅTÄ°RÄ°LMÄ°Å)
# ---------------------------------------------------
def extract_messages_from_soup(soup):
    messages = []

    # Ã–nce bilinen DH mesaj kutularÄ±nÄ± dene
    candidates = []
    candidates += soup.select("span.msg")

    # duplicate bloklarÄ± engelle
    seen_blocks = set()
    filtered_blocks = []

    for c in candidates:
        key = str(c)[:200]
        if key not in seen_blocks:
            seen_blocks.add(key)
            filtered_blocks.append(c)

    # her bloktan gerÃ§ek mesajlarÄ± al
    for block in filtered_blocks:
        block = remove_quotes(block)  # *** Ã¶nemli yeni adÄ±m ***

        td = block.find("td")
        if td:
            text = td.get_text(separator=" ", strip=True)
        else:
            text = block.get_text(separator=" ", strip=True)

        text = clean_text(text)

        if text and len(text) > 3:
            messages.append(text)

    # tekrar eden metinleri sil
    unique = []
    seen = set()

    for m in messages:
        if m not in seen:
            seen.add(m)
            unique.append(m)

    return unique


# ---------------------------------------------------
#   THREAD PARSE FONKSÄ°YONU (ESKÄ°YLE AYNI)
# ---------------------------------------------------
def parse_thread(url):
    try:
        r = requests.get(url, headers=HEADERS, timeout=12)
        r.raise_for_status()
    except Exception as e:
        print(f"âŒ {url} yÃ¼klenemedi: {e}")
        return []

    soup = BeautifulSoup(r.text, "html.parser")

    pages = get_thread_pages(soup, url)
    all_msgs = []

    for page_url in pages:
        try:
            r2 = requests.get(page_url, headers=HEADERS, timeout=12)
            r2.raise_for_status()
        except Exception as e:
            print(f"âš ï¸ Sayfa yÃ¼klenemedi: {page_url}")
            continue

        soup2 = BeautifulSoup(r2.text, "html.parser")
        msgs = extract_messages_from_soup(soup2)

        for m in msgs:
            if m not in all_msgs:
                all_msgs.append(m)

        sleep(0.4)

    return all_msgs


# ---------------------------------------------------
#   CSV'DEN KONULARI OKU (ESKÄ°YLE AYNI)
# ---------------------------------------------------
def load_topics_from_csv(path):
    topics = []
    with open(path, newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            baslik = row.get("BaÅŸlÄ±k")
            link = row.get("Link")
            if baslik and link:
                topics.append({"baslik": baslik, "link": link})
    return topics


# ---------------------------------------------------
#   ANA PROGRAM
# ---------------------------------------------------
def main():
    topics = load_topics_from_csv(KONULAR_CSV)
    print(f"Toplam {len(topics)} konu bulundu.\n")

    rows = []

    for i, t in enumerate(topics, 1):
        print(f"[{i}/{len(topics)}] Ä°ÅŸleniyor â†’ {t['baslik']}")
        msgs = parse_thread(t["link"])

        if not msgs:
            print("âš ï¸ Mesaj alÄ±namadÄ±.\n")
            continue

        sorun = msgs[0]
        cevaplar = msgs[1:]

        rows.append({
            "baslik": t["baslik"],
            "sorun": sorun,
            "cevaplar": " ||| ".join(cevaplar)
        })

        print(f"   âœ” Toplam mesaj: {len(msgs)}\n")
        sleep(1)

    with open(OUT_CSV, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["baslik", "sorun", "cevaplar"])
        writer.writeheader()
        for r in rows:
            writer.writerow(r)

    print(f"\nğŸ’¾ TamamlandÄ± â†’ {OUT_CSV}")


if __name__ == "__main__":
    main()
